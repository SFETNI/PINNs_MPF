{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.optimize import minimize\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyDOE in /home/selfetni/anaconda3/lib/python3.9/site-packages (0.3.8)\n",
      "Requirement already satisfied: numpy in /home/selfetni/anaconda3/lib/python3.9/site-packages (from pyDOE) (1.21.5)\n",
      "Requirement already satisfied: scipy in /home/selfetni/anaconda3/lib/python3.9/site-packages (from pyDOE) (1.9.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 17:10:12.634835: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 17:10:12.710467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-03 17:10:12.710483: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-03 17:10:13.128096: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 17:10:13.128138: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 17:10:13.128142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "#!/home/selfetni/anaconda3/bin/python3.9.19\n",
    "#print(\"PYTHON VERSION: \",sys.version)\n",
    "# Install pyDOE using pip\n",
    "subprocess.call(['pip', 'install', 'pyDOE'])\n",
    "\n",
    "#!pip install pyDOE\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "#hide tf logs \n",
    "os.environ['TF_CPPclea_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'} \n",
    "#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import seaborn as sns \n",
    "import codecs, json\n",
    "import math\n",
    "from numba import jit\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "import random\n",
    "import datetime\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "import re \n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from importlib import reload\n",
    "import PINN  # python files (classes)\n",
    "import pre_post\n",
    "from pre_post import *\n",
    "from PINN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_circles without overlap\n",
    "@jit(nopython=True)\n",
    "def generate_circles(mean_r, num_circles, std, Nx, Ny, Nz):\n",
    "    # Initialize the arrays for the radii and centers of the circles\n",
    "    R0 = np.zeros(num_circles)\n",
    "    X_center = np.zeros(num_circles)\n",
    "    Y_center = np.zeros(num_circles)\n",
    "    Z_center = np.zeros(num_circles)\n",
    "\n",
    "    # Generate the first circle randomly\n",
    "    R0[0] = np.random.normal(loc=mean_r, scale=std)\n",
    "    X_center[0] = np.random.randint(R0[0], Nx-R0[0])\n",
    "    Y_center[0] = np.random.randint(R0[0], Ny-R0[0])\n",
    "    Z_center[0] = np.random.randint(R0[0], Nz-R0[0])\n",
    "\n",
    "    # Loop through the remaining circles and generate them one at a time\n",
    "    for i in range(1, num_circles):\n",
    "        # Flag to indicate whether the new circle overlaps with any existing circles\n",
    "        overlaps = True\n",
    "        while overlaps:\n",
    "            # Generate the radius and center of the new circle randomly\n",
    "            R0[i] = np.random.normal(loc=mean_r, scale=std)\n",
    "            X_center[i] = np.random.randint(R0[i], Nx-R0[i])\n",
    "            Y_center[i] = np.random.randint(R0[i], Ny-R0[i])\n",
    "            Z_center[i] = np.random.randint(R0[i], Nz-R0[i])\n",
    "\n",
    "            # Check the new circle against the existing circles\n",
    "            overlaps = False\n",
    "            for j in range(i):\n",
    "                if np.sqrt((X_center[i]-X_center[j])**2 + (Y_center[i]-Y_center[j])**2 ) < (R0[i]+R0[j]): #+ (Z_center[i]-Z_center[j])**2\n",
    "                    overlaps = True\n",
    "                    break\n",
    "    \n",
    "    return R0, X_center, Y_center, Z_center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_f_train: (45000000, 3), X_ini_train: (3600, 3), X_lb_train: (500, 3), X_ub_train: (500, 3), phi_ini_train: (3600, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 11:56:23.286529: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.286578: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.286608: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.286636: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.286662: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.286689: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.286715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.286741: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-06-19 11:56:23.287051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create an empty list to store u_pred values\n",
    "    # Input parameters\n",
    "    # Grid parameters\n",
    "    Nx=64\n",
    "    Ny=64\n",
    "    Nt=100\n",
    "    eta=0.075  \n",
    "    # Define the domain bounds\n",
    "    lb = np.array([0, 0,0])\n",
    "    ub = np.array([1, 1,1]) #np.array([Nx, Ny,Nt])\n",
    "    \n",
    "    # Phyiscal paramters\n",
    "    v_n=0.5\n",
    "    sigma=1\n",
    "    mu=1e-3\n",
    "    delta_g=-100\n",
    "    \n",
    "    # Numerical parameters \n",
    "    N_batches=36 # Total number of data points for 'phi': boundary \n",
    "    Nbr_pts_max_per_batch=12500  # maximum number  Collocation points per batch for Scipy optimizer \n",
    "    scipy_min_f_pts_per_batch = 500 # minimum number of Collocation points per batch for Scipy optimizer \n",
    "    max_ic_scipy_pts=75 #maximum number of IC points per batch for Scipy optimizer \n",
    "    N_ini_min_per_batch=10\n",
    "    \n",
    "    \n",
    "    scipy_min_f_pts_per_batch_thresh =0.05  # to delete\n",
    "    ic_scipy_thresh=0.05 # \n",
    "    \n",
    "    \n",
    "\n",
    "    num_train_intervals=100\n",
    "    # Define  Collocations, IC and BC points and Domain bounds\n",
    "    N_ini =N_batches *num_train_intervals # Total number of data points for 'phi': IC\n",
    "    N_f = Nbr_pts_max_per_batch * N_batches *num_train_intervals    # 100000 Total number of collocation points : domain\n",
    "    N_b=500   # Total number of data points for boundary BC\n",
    "    N_ini_min_per_batch=3\n",
    "    \n",
    "    \n",
    "        \n",
    "   # Total number of data points for 'phi': boundary BC\n",
    "\n",
    "    # get radius and coordinates\n",
    "    R0, X_center, Y_center,Z_center =\\\n",
    "          generate_circles(mean_r=0.3,num_circles=1, std=0, Nx=Nx, Ny=Ny,Nz=100)\n",
    "    X_center, Y_center=[0.5],[0.5] # single grain\n",
    "\n",
    "    x = np.linspace(lb[0], ub[0], Nx),\n",
    "    y = np.unique(np.linspace(lb[1], ub[1], Ny))\n",
    "    t = np.unique(np.linspace(lb[2], ub[2], Nt))\n",
    "    x=np.expand_dims(x, axis=1)\n",
    "    y=np.expand_dims(y, axis=1)\n",
    "    t=np.expand_dims(t, axis=1)\n",
    "    X, Y,T= np.meshgrid(x,y,t)\n",
    "\n",
    "    tb = np.linspace(start=lb[2], stop=ub[2], num=N_b, endpoint=True)\n",
    "    tb = np.expand_dims(tb, axis=1)\n",
    "    \n",
    "    # set the saving paths and erase older results\n",
    "    global pathOutput\n",
    "    pathOutput = os.path.join(os.getcwd(),'save_figs')\n",
    "    if not os.path.isdir(pathOutput):\n",
    "        os.mkdir(pathOutput)\n",
    "    # to store the weights for each time interval \n",
    "    path_weights= os.path.join(os.getcwd(),'weights')\n",
    "    if not os.path.isdir(path_weights):\n",
    "        os.mkdir(path_weights)\n",
    "\n",
    "    # load PrePost class\n",
    "    reload(pre_post) # for re-execution after modif\n",
    "    from pre_post import *\n",
    "    Pre_Post=PrePost( X=X,T=None, lb=lb, ub=ub, Nx=Nx,Ny=Ny,x=x,y=y, eta=eta,\\\n",
    "        phi_true=None,R0=R0)\n",
    "\n",
    "    # set the save paths and erase older results\n",
    "    #Pre_Post.EraseFile(path=pathOutput)\n",
    "    #Pre_Post.EraseFile(path=path_weights)\n",
    " \n",
    "    # get phi_0\n",
    "    phi_0, X_ini_all=Pre_Post.init_micro_cir(X_center,Y_center, Z_center,N_ini,Nx,Ny,x,y,lb,ub) \n",
    "    \n",
    "    # plot the true solution\n",
    "    #Pre_Post.plot_exact(path=pathOutput)\n",
    "\n",
    "    # plot the initial micro\n",
    "    Pre_Post.plot_init(X_ini_all,phi_0,Nx,Ny,path=pathOutput)\n",
    "    \n",
    "    # get the training data\n",
    "    X_f_train, X_ini_train,X_lb_train,X_ub_train,X_rtb_train,X_ltb_train,\\\n",
    "        phi_ini_train, X_ini_train_all, phi_ini_train_all=Pre_Post.set_training_data(x,y,X_ini_all,N_ini,phi_0,N_f,tb,lb,ub)\n",
    " \n",
    "    # Plot Collocation_IC_BC points\n",
    "    #Pre_Post.plot_Collocation_IC_BC(Nx,Ny,x,y,X_ini_train,X_f_train,X_lb_train,X_ub_train,\\\n",
    "    #                                X_rtb_train,X_ltb_train,phi_0,phi_ini_train,path=pathOutput)\n",
    "      \n",
    "    # Testing spatio-temporal domain\n",
    "    X_phi_test = np.hstack((X.flatten()[:,None],Y.flatten()[:,None], T.flatten()[:,None])) \n",
    "    #tf.print(X_phi_test)\n",
    " \n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PINN class\n",
    "import PINN\n",
    "reload(PINN)  # mandatory to reload content at each re-call atfer modification\n",
    "from PINN import *\n",
    "\n",
    "# Build PINN \n",
    "# Build PINN \n",
    "layers = np.array([3,128,128, 128,128,128,128,1])  # Network\n",
    "PINN_ = Sequentialmodel(layers=layers, X_f_train=X_f_train, X_ini_train=X_ini_train,\\\n",
    "                        X_lb_train=X_lb_train, X_ub_train=X_ub_train,\\\n",
    "                        X_ltb_train=X_ltb_train, X_rtb_train=X_rtb_train,\\\n",
    "                        phi_0=phi_0,phi_ini_train=phi_ini_train, N_ini=N_ini,X_u_test=X_phi_test,\\\n",
    "                        X_ini_train_all=X_ini_train_all, phi_ini_train_all=phi_ini_train_all,\\\n",
    "                        X=X,T=T,x=x,y=y,lb=lb, ub=ub, mu=mu, sigma=sigma, delta_g=delta_g,\\\n",
    "                                    R0=R0,X_center=X_center,Y_center=Y_center,eta=eta,Nx=Nx,Ny=Ny,Nt=Nt,phi_sol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_.save_predictions(1,pathOutput,PINN_.X_u_test, PINN_.X_ini,PINN_.phi_ini,N_b,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check  4 4\n"
     ]
    }
   ],
   "source": [
    "N_batches= 16 \n",
    "limits = []\n",
    "num_x_intervals = int(np.ceil(np.sqrt(N_batches)))\n",
    "num_y_intervals = int(np.ceil(np.sqrt(N_batches)))\n",
    "\n",
    "step_x = 1 / num_x_intervals\n",
    "step_y = 1/ num_y_intervals\n",
    "for i in range(int(np.sqrt(N_batches))):\n",
    "    for j in range(int(np.sqrt(N_batches))):\n",
    "        #tf.print(\"index\", index)\n",
    "        x_min = i * step_x\n",
    "        x_max = (i + 1) * step_x\n",
    "        y_min = j * step_y\n",
    "        y_max = (j + 1) * step_y\n",
    "        limits.append([x_min, x_max, y_min, y_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.25, 0.0, 0.25],\n",
       " [0.0, 0.25, 0.25, 0.5],\n",
       " [0.0, 0.25, 0.5, 0.75],\n",
       " [0.0, 0.25, 0.75, 1.0],\n",
       " [0.25, 0.5, 0.0, 0.25],\n",
       " [0.25, 0.5, 0.25, 0.5],\n",
       " [0.25, 0.5, 0.5, 0.75],\n",
       " [0.25, 0.5, 0.75, 1.0],\n",
       " [0.5, 0.75, 0.0, 0.25],\n",
       " [0.5, 0.75, 0.25, 0.5],\n",
       " [0.5, 0.75, 0.5, 0.75],\n",
       " [0.5, 0.75, 0.75, 1.0],\n",
       " [0.75, 1.0, 0.0, 0.25],\n",
       " [0.75, 1.0, 0.25, 0.5],\n",
       " [0.75, 1.0, 0.5, 0.75],\n",
       " [0.75, 1.0, 0.75, 1.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FD resolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
